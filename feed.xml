<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://nabin2004.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nabin2004.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-01T17:11:44+00:00</updated><id>https://nabin2004.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Self Attention Mechanism</title><link href="https://nabin2004.github.io/blog/2024/self-attention/" rel="alternate" type="text/html" title="Self Attention Mechanism"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://nabin2004.github.io/blog/2024/self-attention</id><content type="html" xml:base="https://nabin2004.github.io/blog/2024/self-attention/"><![CDATA[<p>The world took notice in November 2022 when OpenAI released ChatGPT. Based on the Transformer architecture, it continues to power countless applications today. This architecture originates from the paper <em>“Attention Is All You Need”</em>, and though it has been modified in many ways since, the core idea—attention—remains central.</p> <h2 id="why-did-transformers-become-so-successful">Why Did Transformers Become So Successful?</h2> <p>Before answering that, a quick note: this is the first post in a Transformer series. I plan to cover components like multi-head attention, positional encodings, architecture breakdowns, and possibly pretraining, RLHF, and fine-tuning. All that, if I stay consistent. Because, yes—<strong>Consistent is All I Need</strong>.</p> <p>Before Transformers, RNNs reigned supreme. You can get a feel for their dominance from the tongue-in-cheek paper <em>“RNNs Were All We Needed”</em>, or Karpathy’s classic blog <em>“The Unreasonable Effectiveness of Recurrent Neural Networks”</em>.</p> <h3 id="but-rnns-had-problems">But RNNs had problems:</h3> <h4 id="bottlenecks">Bottlenecks</h4> <ul> <li>RNNs process one token at a time—each step depends on the previous one.</li> <li>This kills parallelism. GPUs can’t help much here.</li> <li>Training is slow, inference is worse.</li> </ul> <h4 id="vanishingexploding-gradients">Vanishing/Exploding Gradients</h4> <ul> <li>Gradients can shrink to nothing or explode on long sequences.</li> <li>LSTMs and GRUs helped, but didn’t solve it completely.</li> <li>If a token appeared 40 steps ago, the RNN might forget it like last Tuesday’s lunch.</li> </ul> <h4 id="scaling-limitations">Scaling Limitations</h4> <ul> <li>Stack many RNN layers, try long documents, and watch training time explode and quality suffer.</li> </ul> <hr/> <h2 id="what-we-wanted">What We Wanted</h2> <ul> <li>See the entire sequence at once</li> <li>Learn which tokens matter (regardless of position)</li> <li>Parallelize and scale efficiently</li> <li>Handle long-term dependencies without forgetting the start</li> </ul> <hr/> <h2 id="enter-self-attention">Enter: Self-Attention</h2> <p>What if each token could look at every other token and decide how important each one is?</p> <p>Self-attention allows a model to <strong>weigh the relevance of all tokens</strong> when encoding any specific one.</p> <p>Let’s walk through a metaphor:</p> <blockquote> <p>Meet Tok. Tok is a token.<br/> Last week, Tok lived in an RNN. It waited patiently, only to be forgotten by the time the model got to it.<br/> Today, Tok lives the dream. A world where every token sees every other token—instantly.<br/> No waiting. No forgetting. Tok gets attention not for being first or last, but for being <strong>relevant</strong>.<br/> You, the reader, understood “he” referred to Tok. How? You too attend globally, just like self-attention.</p> </blockquote> <p>Self-attention gives every token a <strong>global view</strong>. It computes attention scores between all token pairs, deciding who matters to whom. The result? A new, context-aware representation for every token.</p> <hr/> <h2 id="if-you-understand-python-dictionaries-youre-halfway-there">If You Understand Python Dictionaries, You’re Halfway There</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">names</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">first</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">nabin</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">second</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">oli</span><span class="sh">'</span><span class="p">}</span>
</code></pre></div></div> <p>In self-attention, we’re doing something similar: querying over keys to retrieve values. Except now, it’s all vectorized.</p> <h3 id="the-core-elements">The Core Elements:</h3> <ul> <li><strong>Q</strong> (Query): What I’m looking for</li> <li><strong>K</strong> (Key): What others are offering</li> <li><strong>V</strong> (Value): What I’ll take if the offer matches</li> </ul> <p>Each token has its own Q, K, and V. Attention is matching Qs with Ks to decide which Vs to blend in.</p> <hr/> <h2 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h2> <p>This is where Q, K, and V interact.</p> <ol> <li>Take the dot product between a token’s Query and every other Key.</li> <li>This gives a score of how well each other token “matches” the query.</li> <li>Do this for every token pair — you get an n × n matrix of scores.</li> <li>To stabilize large values (from high-dimensional vectors), divide by √dₖ (dimension of Key).</li> <li>Apply softmax to turn scores into probabilities.</li> <li>Use these weights to compute a weighted sum of the Value vectors.</li> </ol> <h3 id="why-scale">Why scale?</h3> <p>Without scaling, large dot products can push softmax into extreme values—leading to unstable training.</p> <p><strong>Summary:</strong> Dot product = similarity. Scaling = numerical stability. Softmax = who actually matters. Weighted sum = updated token representation.</p> <hr/> <h2 id="a-worked-example">A Worked Example</h2> <p>Given:</p> <ul> <li>“The” = [0.1, 0.2]</li> <li>“cat” = [0.3, 0.4]</li> <li>“sat” = [0.5, 0.6]</li> </ul> <p>Suppose attention weights for “cat” are:</p> <ul> <li>10% to “The”</li> <li>40% to itself</li> <li>50% to “sat”</li> </ul> <p>Then its updated vector is:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>= 0.10 * [0.1, 0.2] +
  0.40 * [0.3, 0.4] +
  0.50 * [0.5, 0.6]

= [0.38, 0.48]
</code></pre></div></div> <p>The “cat” now embeds information from the whole sentence.</p> <hr/> <h2 id="why-does-it-work">Why Does It Work?</h2> <ul> <li><strong>Global context:</strong> Every token sees every other token.</li> <li><strong>Handles long-range dependencies:</strong> “He” can refer back to “Tok”, no matter how far away.</li> <li><strong>GPU-friendly and parallelizable:</strong> Unlike RNNs, attention works all at once.</li> </ul> <hr/> <h2 id="frequently-asked-questions">Frequently Asked Questions</h2> <p><strong>Q: Where do Q, K, V come from?</strong> A: Each token is embedded into a vector. From that, we use learned weight matrices to produce Q, K, and V:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Q = embedding × Wq  
K = embedding × Wk  
V = embedding × Wv
</code></pre></div></div> <p><strong>Q: Why separate Q, K, and V?</strong> A: Each has a different job—asking, identifying, and providing. Using one vector for all three is like asking one person to be the detective, witness, and evidence.</p> <p><strong>Q: How does the model learn Wq, Wk, Wv?</strong> A: Like any neural net—via backpropagation during training.</p> <p><strong>Q: If attention looks at everything, how does it know the order of words?</strong> A: Transformers add <strong>positional encodings</strong>, covered in a future post.</p> <p><strong>Q: Can attention handle very long sequences?</strong> A: Technically yes, but vanilla attention scales with O(n²). Beyond a few thousand tokens, it becomes expensive. There are efficient variants.</p> <p><strong>Q: How is this better than RNNs or CNNs?</strong> A: RNNs are slow and sequential. CNNs are limited to local context. Attention is global, parallel, and distance-agnostic.</p> <p><strong>Q: What happens if we don’t scale the dot product?</strong> A: Softmax becomes unstable with large values, leading to bad gradients and poor training.</p> <p><strong>Q: Is this too heavy for small devices?</strong> A: Vanilla self-attention is computationally heavy. Lightweight or sparse variants exist for edge devices.</p> <p><strong>Q: What about other domains like images or audio?</strong> A: Transformers work there too. Vision Transformers treat patches as tokens; audio models treat frames as tokens.</p> <hr/> <h2 id="tldr">TL;DR</h2> <ul> <li>Ask the right questions (<strong>Q</strong>)</li> <li>Match them with the right sources (<strong>K</strong>)</li> <li>Listen to the right information (<strong>V</strong>)</li> <li>Decide who to trust (softmax)</li> <li>Then act (weighted sum of values)</li> </ul> <hr/> <p>Stay tuned for the next post, where we’ll dive into <strong>Multi-Head Attention</strong> and <strong>Positional Encoding</strong>.</p>]]></content><author><name></name></author><category term="transformer"/><category term="transformers,"/><category term="deep"/><category term="learning,"/><category term="attention,"/><category term="NLP"/><summary type="html"><![CDATA[Self-Attention Mechanism]]></summary></entry></feed>